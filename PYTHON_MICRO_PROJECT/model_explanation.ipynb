{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "heading-title",
   "metadata": {},
   "source": [
    "# Ethereum Fraud Detection System\n",
    "\n",
    "This notebook provides comprehensive documentation of our Ethereum fraud detection system, which uses machine learning to identify potentially fraudulent transactions based on wallet behavior patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project-overview",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "Our fraud detection system consists of two main components:\n",
    "1. A model training pipeline (`model.py`)\n",
    "2. A prediction system for new wallets (`predict.py`)\n",
    "\n",
    "The system uses a Random Forest classifier trained on blockchain transaction data to identify patterns associated with fraudulent activities in the Ethereum network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-code",
   "metadata": {},
   "source": [
    "## 1. Model Training (`model.py`)\n",
    "\n",
    "The `model.py` script implements a complete machine learning pipeline for fraud detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, classification_report\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('csv_files/transaction_dataset.csv', index_col=0)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Handle categorical columns\n",
    "categories = df.select_dtypes('O').columns\n",
    "print(f\"Dropping categorical columns: {list(categories)}\")\n",
    "df.drop(df[categories], axis=1, inplace=True)\n",
    "\n",
    "# Fill missing values with median\n",
    "df.fillna(df.median(), inplace=True)\n",
    "\n",
    "# Remove features with zero variance\n",
    "no_var = df.var() == 0\n",
    "if any(no_var):\n",
    "    print(f\"Dropping zero variance features: {list(df.var()[no_var].index)}\")\n",
    "    df.drop(df.var()[no_var].index, axis=1, inplace=True)\n",
    "\n",
    "# Drop redundant or less important features\n",
    "drop = ['total transactions (including tnx to create contract', 'total ether sent contracts', \n",
    "        'max val sent to contract', ' ERC20 avg val rec', ' ERC20 max val rec', ' ERC20 min val rec', \n",
    "        ' ERC20 uniq rec contract addr', 'max val sent', ' ERC20 avg val sent', ' ERC20 min val sent', \n",
    "        ' ERC20 max val sent', ' Total ERC20 tnxs', 'avg value sent to contract', 'Unique Sent To Addresses',\n",
    "        'Unique Received From Addresses', 'total ether received', ' ERC20 uniq sent token name', \n",
    "        'min value received', 'min val sent', ' ERC20 uniq rec addr', 'min value sent to contract', \n",
    "        ' ERC20 uniq sent addr.1']\n",
    "\n",
    "# Only drop columns that exist in the dataframe\n",
    "drop_existing = [col for col in drop if col in df.columns]\n",
    "if drop_existing:\n",
    "    df.drop(drop_existing, axis=1, inplace=True)\n",
    "    print(f\"Dropped {len(drop_existing)} redundant features\")\n",
    "\n",
    "# Display updated dataset shape\n",
    "print(f\"Dataset shape after cleaning: {df.shape}\")\n",
    "\n",
    "# Split features and target\n",
    "y = df['FLAG']\n",
    "X = df.drop('FLAG', axis=1)\n",
    "print(f\"Features: {X.shape}, Target: {y.shape}\")\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=123)\n",
    "print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "\n",
    "# Normalize the features\n",
    "norm = PowerTransformer()\n",
    "norm_train_f = norm.fit_transform(X_train)\n",
    "norm_test_f = norm.transform(X_test)\n",
    "\n",
    "# Before SMOTE - Class distribution\n",
    "print(f\"Before SMOTE - Class distribution: {np.bincount(y_train)}\")\n",
    "fraud_nonfraud_before = np.bincount(y_train)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(fraud_nonfraud_before, labels=['Non-Fraud', 'Fraud'], autopct='%1.1f%%', startangle=90, colors=['skyblue', 'orange'])\n",
    "plt.title('Fraud vs Non-Fraud Distribution (Before SMOTE)')\n",
    "plt.savefig('images/fraud_distribution_before_smote.png')\n",
    "plt.show()\n",
    "\n",
    "# Apply SMOTE to handle class imbalance\n",
    "oversample = SMOTE()\n",
    "x_tr_resample, y_tr_resample = oversample.fit_resample(norm_train_f, y_train)\n",
    "print(f\"After SMOTE - Class distribution: {np.bincount(y_tr_resample)}\")\n",
    "\n",
    "# After SMOTE - Class distribution\n",
    "fraud_nonfraud_after = np.bincount(y_tr_resample)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(fraud_nonfraud_after, labels=['Non-Fraud', 'Fraud'], autopct='%1.1f%%', startangle=90, colors=['skyblue', 'orange'])\n",
    "plt.title('Fraud vs Non-Fraud Distribution (After SMOTE)')\n",
    "plt.savefig('images/fraud_distribution_after_smote.png')\n",
    "plt.show()\n",
    "\n",
    "# Train Random Forest model\n",
    "print(\"Training Random Forest model...\")\n",
    "RF = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "RF.fit(x_tr_resample, y_tr_resample)\n",
    "\n",
    "# Make predictions\n",
    "preds_RF = RF.predict(norm_test_f)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(classification_report(y_test, preds_RF))\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, preds_RF)}\")\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, preds_RF)}\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': RF.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance.head(10))\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('images/feature_importance.png')\n",
    "\n",
    "# Save the model\n",
    "model_filename = 'models/ethereum_fraud_model.pkl'\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(RF, file)\n",
    "print(f\"\\nModel saved as {model_filename}\")\n",
    "\n",
    "# Save the scaler for future predictions\n",
    "scaler_filename = 'models/ethereum_fraud_scaler.pkl'\n",
    "with open(scaler_filename, 'wb') as file:\n",
    "    pickle.dump(norm, file)\n",
    "print(f\"Scaler saved as {scaler_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-explanation",
   "metadata": {},
   "source": [
    "### Model Training Pipeline Walkthrough\n",
    "\n",
    "The `model.py` script implements a comprehensive machine learning workflow for fraud detection in Ethereum transactions:\n",
    "\n",
    "1. **Data Loading and Preprocessing**\n",
    "   - Loads transaction data from a CSV file\n",
    "   - Removes categorical features that can't be directly used for modeling\n",
    "   - Handles missing values by filling with median values\n",
    "   - Removes features with zero variance that provide no discriminative information\n",
    "   - Drops redundant or less important features based on domain knowledge\n",
    "\n",
    "2. **Data Preparation**\n",
    "   - Splits the data into features (X) and target (y)\n",
    "   - Further splits into training and testing sets (60/40 split)\n",
    "   - Normalizes features using PowerTransformer for better model performance\n",
    "\n",
    "3. **Class Imbalance Handling**\n",
    "   - Visualizes the class distribution before resampling (typically highly imbalanced in fraud detection)\n",
    "   - Applies SMOTE (Synthetic Minority Oversampling Technique) to create synthetic samples of the minority class\n",
    "   - Visualizes the balanced class distribution after SMOTE\n",
    "\n",
    "4. **Model Training and Evaluation**\n",
    "   - Trains a Random Forest classifier with 100 trees\n",
    "   - Makes predictions on the test set\n",
    "   - Evaluates model using classification metrics (precision, recall, F1-score)\n",
    "   - Generates confusion matrix and ROC AUC score\n",
    "\n",
    "5. **Feature Importance Analysis**\n",
    "   - Extracts feature importance scores from the Random Forest model\n",
    "   - Visualizes the top 10 most important features\n",
    "\n",
    "6. **Model Persistence**\n",
    "   - Saves the trained model and scaler for future use\n",
    "   - These files will be used by the prediction script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predict-script",
   "metadata": {},
   "source": [
    "## 2. Prediction System (`predict.py`)\n",
    "\n",
    "The `predict.py` script uses the trained model to make predictions on new wallet data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prediction-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def predict_fraud(wallet_data_path='csv_files/wallet_data.csv'):\n",
    "    \"\"\"\n",
    "    Make fraud predictions on wallet data using the pre-trained model\n",
    "    \n",
    "    Parameters:\n",
    "    wallet_data_path (str): Path to CSV file containing wallet data\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with wallet addresses and fraud predictions\n",
    "    \"\"\"\n",
    "    # Load the pre-trained model and scaler\n",
    "    try:\n",
    "        print(\"Loading pre-trained model and scaler...\")\n",
    "        with open('models/ethereum_fraud_model.pkl', 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        with open('models/ethereum_fraud_scaler.pkl', 'rb') as f:\n",
    "            scaler = pickle.load(f)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Ensure that the model and scaler files exist in the 'models' directory.\")\n",
    "        return\n",
    "    \n",
    "    # Load wallet data\n",
    "    print(f\"Loading wallet data from {wallet_data_path}...\")\n",
    "    wallet_data = pd.read_csv(wallet_data_path, index_col=0)\n",
    "    print(f\"Wallet data shape: {wallet_data.shape}\")\n",
    "    \n",
    "    \n",
    "    # Store wallet addresses if present\n",
    "    addresses = None\n",
    "    if 'Address' in wallet_data.columns:\n",
    "        addresses = wallet_data['Address'].copy()\n",
    "        wallet_data = wallet_data.drop('Address', axis=1)\n",
    "    \n",
    "    # Handle missing values\n",
    "    wallet_data.fillna(wallet_data.median(), inplace=True)\n",
    "    \n",
    "    # Drop FLAG column if present (for testing purposes)\n",
    "    true_labels = None\n",
    "    if 'FLAG' in wallet_data.columns:\n",
    "        true_labels = wallet_data['FLAG'].copy()\n",
    "        wallet_data = wallet_data.drop('FLAG', axis=1)\n",
    "    \n",
    "    # Align feature names with training data\n",
    "    required_features = scaler.feature_names_in_  # Features used during training\n",
    "    for feature in required_features:\n",
    "        if feature not in wallet_data.columns:\n",
    "            wallet_data[feature] = 0  # Add missing features with default value 0\n",
    "    wallet_data = wallet_data[required_features]  # Select only required features\n",
    "    \n",
    "    # Scale features\n",
    "    wallet_data_scaled = scaler.transform(wallet_data)\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"Making predictions...\")\n",
    "    fraud_probs = model.predict_proba(wallet_data_scaled)[:, 1]\n",
    "    fraud_preds = model.predict(wallet_data_scaled)\n",
    "    \n",
    "    # Create results dataframe\n",
    "    if addresses is not None:\n",
    "        results = pd.DataFrame({\n",
    "            'Address': addresses,\n",
    "            'Fraud_Prediction': fraud_preds,\n",
    "            'Fraud_Probability': fraud_probs\n",
    "        })\n",
    "    else:\n",
    "        results = pd.DataFrame({\n",
    "            'Fraud_Prediction': fraud_preds,\n",
    "            'Fraud_Probability': fraud_probs\n",
    "        })\n",
    "    \n",
    "    # Sort by fraud probability\n",
    "    results = results.sort_values('Fraud_Probability', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Save results\n",
    "    results.to_csv('csv_files/fraud_predictions.csv', index=True)\n",
    "    print(\"Predictions saved to 'csv_files/fraud_predictions.csv'\")\n",
    "    \n",
    "    # Display top potential fraudsters\n",
    "    print(results.head(10))\n",
    "    \n",
    "    # Plot fraud probability distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(fraud_probs, bins=50)\n",
    "    plt.title('Distribution of Fraud Probabilities')\n",
    "    plt.xlabel('Probability of Fraud')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig('images/fraud_probability_distribution.png')\n",
    "    \n",
    "    # Evaluate against true labels if available\n",
    "    if true_labels is not None:\n",
    "        from sklearn.metrics import classification_report, confusion_matrix\n",
    "        print(\"\\nModel performance on this dataset:\")\n",
    "        print(classification_report(true_labels, fraud_preds))\n",
    "        print(f\"Confusion Matrix:\\n{confusion_matrix(true_labels, fraud_preds)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predict_fraud()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prediction-explanation",
   "metadata": {},
   "source": [
    "### Prediction System Walkthrough\n",
    "\n",
    "The `predict.py` script provides a streamlined workflow for making predictions on new wallet data:\n",
    "\n",
    "1. **Model Loading**\n",
    "   - Loads the pre-trained Random Forest model and PowerTransformer scaler from pickle files\n",
    "   - Includes error handling to ensure the model files exist\n",
    "\n",
    "2. **Data Loading and Preprocessing**\n",
    "   - Loads wallet data from a CSV file\n",
    "   - Preserves wallet addresses for the final results\n",
    "   - Handles missing values consistently with the training phase\n",
    "   - Optionally preserves true labels if present (for evaluation purposes)\n",
    "\n",
    "3. **Feature Alignment**\n",
    "   - Ensures that the input data has the same features used during training\n",
    "   - Adds missing features with default values\n",
    "   - Selects only the required features in the correct order\n",
    "\n",
    "4. **Prediction Generation**\n",
    "   - Scales the features using the saved PowerTransformer\n",
    "   - Generates both binary predictions (fraud/non-fraud) and fraud probabilities\n",
    "   - Creates a comprehensive results DataFrame\n",
    "\n",
    "5. **Results Processing**\n",
    "   - Sorts results by fraud probability (highest risk first)\n",
    "   - Saves predictions to a CSV file\n",
    "   - Displays the top potential fraudsters\n",
    "\n",
    "6. **Visualization and Evaluation**\n",
    "   - Generates a histogram of fraud probabilities\n",
    "   - If true labels are available, evaluates model performance with metrics like precision, recall, and F1-score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "libraries-analysis",
   "metadata": {},
   "source": [
    "## Core Libraries and Their Roles\n",
    "\n",
    "| Library | Purpose | Key Functions Used |\n",
    "|---------|---------|-------------------|\n",
    "| **Pandas** | Data manipulation and analysis | `read_csv`, `DataFrame`, `drop`, `fillna` |\n",
    "| **NumPy** | Numerical operations | `bincount`, array operations |\n",
    "| **Scikit-learn** | Machine learning functionality | `train_test_split`, `PowerTransformer`, `RandomForestClassifier`, evaluation metrics |\n",
    "| **Imbalanced-learn** | Handling class imbalance | `SMOTE` |\n",
    "| **Matplotlib/Seaborn** | Data visualization | `plt.figure`, `plt.pie`, `sns.barplot`, `sns.histplot` |\n",
    "| **Pickle** | Model persistence | `dump`, `load` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-workflow",
   "metadata": {},
   "source": [
    "## Typical Workflow\n",
    "\n",
    "1. **Training Phase** (run once):\n",
    "   - Run `model.py` to train the fraud detection model\n",
    "   - Review the model evaluation metrics and visualizations\n",
    "   - Analyze feature importance to understand key fraud indicators\n",
    "\n",
    "2. **Prediction Phase** (run as needed):\n",
    "   - Prepare new wallet data in the required format\n",
    "   - Run `predict.py` to generate fraud predictions\n",
    "   - Review the sorted results to identify high-risk wallets\n",
    "   - Analyze the distribution of fraud probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fraud-indicators",
   "metadata": {},
   "source": [
    "## Key Fraud Indicators (Feature Importance)\n",
    "\n",
    "The Random Forest model identifies the most important features for detecting fraudulent wallets. These typically include:\n",
    "\n",
    "1. Transaction frequency patterns\n",
    "2. Value distribution of transactions\n",
    "3. Network connectivity measures\n",
    "4. Temporal behavior patterns\n",
    "5. ERC20 token interaction patterns\n",
    "\n",
    "The feature importance visualization in `model.py` provides specific insights into which features are most predictive in the current dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "system-limitations",
   "metadata": {},
   "source": [
    "## System Limitations and Considerations\n",
    "\n",
    "1. **Class Imbalance**: Fraud detection typically deals with highly imbalanced datasets. While SMOTE helps address this, it creates synthetic data that may not perfectly represent real-world fraud patterns.\n",
    "\n",
    "2. **Feature Engineering**: The system relies on pre-extracted features. Advanced feature engineering could potentially improve performance.\n",
    "\n",
    "3. **Model Updates**: Fraud patterns evolve over time. Regular retraining with new data is recommended.\n",
    "\n",
    "4. **False Positives**: High-risk predictions should be investigated further before taking action, as legitimate wallets may sometimes exhibit unusual patterns.\n",
    "\n",
    "5. **Scalability**: For very large datasets or real-time prediction needs, additional optimizations may be required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This Ethereum fraud detection system provides a robust framework for identifying potentially fraudulent wallet addresses based on transaction patterns. The combination of careful data preprocessing, class imbalance handling, and a Random Forest classifier offers good performance for this challenging task.\n",
    "\n",
    "By leveraging both binary predictions and probability scores, the system enables risk-based prioritization of wallets for further investigation, helping to focus resources on the most suspicious cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
